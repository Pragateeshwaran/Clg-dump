import tensorflow as tf
from tensorflow.keras import layers, models
import matplotlib.pyplot as plt
import numpy as np
from tensorflow.keras.preprocessing.image import ImageDataGenerator



# Device configuration (TensorFlow handles this automatically)



# Hyperparameters
latent_dim = 100
image_size = 128
channels = 3  # RGB
hidden_dim = 64
batch_size = 64
num_epochs = 200
lr = 0.0002
beta1 = 0.5



# Data loading and preprocessing
data_gen = ImageDataGenerator(
    rescale=1./255,
    preprocessing_function=lambda x: (x - 0.5) * 2  # Normalize to [-1, 1]
)
dataset = data_gen.flow_from_directory(
    'lfw',  # Update this path
    target_size=(image_size, image_size),
    batch_size=batch_size,
    class_mode=None,
    shuffle=True
)

# Generator
def build_generator():
    model = models.Sequential([
        layers.Input(shape=(latent_dim,)),
        layers.Reshape((1, 1, latent_dim)),
        layers.Conv2DTranspose(hidden_dim * 16, 4, strides=1, padding='valid', use_bias=False),
        layers.BatchNormalization(), layers.ReLU(),
        layers.Conv2DTranspose(hidden_dim * 8, 4, strides=2, padding='same', use_bias=False),
        layers.BatchNormalization(), layers.ReLU(),
        layers.Conv2DTranspose(hidden_dim * 4, 4, strides=2, padding='same', use_bias=False),
        layers.BatchNormalization(), layers.ReLU(),
        layers.Conv2DTranspose(hidden_dim * 2, 4, strides=2, padding='same', use_bias=False),
        layers.BatchNormalization(), layers.ReLU(),
        layers.Conv2DTranspose(channels, 4, strides=2, padding='same', use_bias=False, activation='tanh')
    ])
    return model

# Discriminator
def build_discriminator():
    model = models.Sequential([
        layers.Input(shape=(image_size, image_size, channels)),
        layers.Conv2D(hidden_dim, 4, strides=2, padding='same', use_bias=False),
        layers.LeakyReLU(0.2),
        layers.Conv2D(hidden_dim * 2, 4, strides=2, padding='same', use_bias=False),
        layers.BatchNormalization(), layers.LeakyReLU(0.2),
        layers.Conv2D(hidden_dim * 4, 4, strides=2, padding='same', use_bias=False),
        layers.BatchNormalization(), layers.LeakyReLU(0.2),
        layers.Conv2D(hidden_dim * 8, 4, strides=2, padding='same', use_bias=False),
        layers.BatchNormalization(), layers.LeakyReLU(0.2),
        layers.Conv2D(1, 4, strides=1, padding='valid', use_bias=False, activation='sigmoid')
    ])
    return model

# Initialize models and optimizers
G = build_generator()
D = build_discriminator()
g_optimizer = tf.keras.optimizers.Adam(lr, beta_1=beta1)
d_optimizer = tf.keras.optimizers.Adam(lr, beta_1=beta1)
loss_fn = tf.keras.losses.BinaryCrossentropy()

# Training step
@tf.function
def train_step(images):
    batch_size = tf.shape(images)[0]
    noise = tf.random.normal([batch_size, latent_dim])
    
    with tf.GradientTape() as g_tape, tf.GradientTape() as d_tape:
        fake = G(noise, training=True)
        real_output = D(images, training=True)
        fake_output = D(fake, training=True)
        
        d_loss_real = loss_fn(tf.ones_like(real_output), real_output)
        d_loss_fake = loss_fn(tf.zeros_like(fake_output), fake_output)
        d_loss = d_loss_real + d_loss_fake
        g_loss = loss_fn(tf.ones_like(fake_output), fake_output)
    
    d_grads = d_tape.gradient(d_loss, D.trainable_variables)
    g_grads = g_tape.gradient(g_loss, G.trainable_variables)
    d_optimizer.apply_gradients(zip(d_grads, D.trainable_variables))
    g_optimizer.apply_gradients(zip(g_grads, G.trainable_variables))
    return d_loss, g_loss


# Training loop
step = 0
for epoch in range(num_epochs):
    for images in dataset:
        step += 1
        d_loss, g_loss = train_step(images)
        
        if step % (len(dataset) // 10) == 0:  # Visualize every ~10% of epoch
            print(f'Epoch [{epoch}/{num_epochs}] Step [{step}] D_loss: {d_loss:.4f} G_loss: {g_loss:.4f}')
            noise = tf.random.normal([16, latent_dim])
            fake = G(noise, training=False)
            fake = (fake + 1) / 2  # Rescale to [0, 1]
            grid = tf.transpose(tf.make_ndarray(tf.make_tensor_proto(fake)), [0, 2, 1, 3])
            grid = np.reshape(grid, (4, 4, image_size, image_size, channels))
            grid = np.transpose(np.concatenate(np.concatenate(grid, axis=1), axis=1), (1, 0, 2))
            plt.imshow(grid)
            plt.axis('off')
            plt.show()
        
        if step >= len(dataset):
            break
    
    # Save checkpoints
    if epoch % 10 == 0:
        G.save_weights(f'generator_epoch_{epoch}.h5')
        D.save_weights(f'discriminator_epoch_{epoch}.h5')

# Generate final samples
noise = tf.random.normal([8, latent_dim])
fake_images = G(noise, training=False)
fake_images = (fake_images + 1) / 2  # Rescale to [0, 1]
grid = tf.transpose(tf.make_ndarray(tf.make_tensor_proto(fake_images)), [0, 2, 1, 3])
grid = np.reshape(grid, (2, 4, image_size, image_size, channels))
grid = np.transpose(np.concatenate(np.concatenate(grid, axis=1), axis=1), (1, 0, 2))
plt.imshow(grid)
plt.axis('off')
plt.show()



import tensorflow as tf
import pandas as pd
from transformers import BertTokenizer, TFBertModel
import numpy as np

# Step 1: Load Pretrained BERT Model
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
bert_model = TFBertModel.from_pretrained('bert-base-uncased')

# Step 2: Student Details Table (Simulated Dataset)
data = {
    'Student_ID': [101, 102, 103],
    'Name': ['Alice', 'Bob', 'Charlie'],
    'Age': [20, 21, 22],
    'Course': ['Computer Science', 'Mathematics', 'Physics'],
    'Marks': [85, 78, 92],
    'Grade': ['A', 'B', 'A+']
}
df = pd.DataFrame(data)

# Convert table to searchable text format
student_texts = [
    f"{row['Name']} is {row['Age']} years old, enrolled in {row['Course']}, scored {row['Marks']} marks, and has grade {row['Grade']}."
    for _, row in df.iterrows()
]

# Step 3: BERT Embeddings for Student Texts
def get_bert_embedding(text):
    inputs = tokenizer(text, return_tensors='tf', padding=True, truncation=True, max_length=128)
    outputs = bert_model(inputs)
    return tf.reduce_mean(outputs.last_hidden_state, axis=1).numpy()  # Mean pooling

student_embeddings = np.vstack([get_bert_embedding(text) for text in student_texts])

# Step 4: Query Understanding and Response Generation
def get_query_embedding(query):
    inputs = tokenizer(query, return_tensors='tf', padding=True, truncation=True, max_length=128)
    outputs = bert_model(inputs)
    return tf.reduce_mean(outputs.last_hidden_state, axis=1).numpy()

def find_best_match(query_embedding):
    similarities = np.dot(student_embeddings, query_embedding.T).flatten()
    return np.argmax(similarities)

def extract_answer(query, text):
    query_lower = query.lower()
    if 'marks' in query_lower:
        return f"{text.split('scored ')[1].split(' marks')[0]} marks"
    elif 'course' in query_lower:
        return text.split('enrolled in ')[1].split(',')[0]
    elif 'highest' in query_lower and 'physics' in query_lower:
        physics_students = df[df['Course'] == 'Physics']
        max_marks = physics_students['Marks'].max()
        top_student = physics_students[physics_students['Marks'] == max_marks]['Name'].values[0]
        return f"{top_student} scored the highest in Physics with {max_marks} marks"
    elif 'grade a' in query_lower:
        a_students = df[df['Grade'] == 'A'][['Name', 'Marks', 'Course']].to_string(index=False)
        return f"Students with Grade A:\n{a_students}"
    return text  # Default: return full student info

# Step 5: Chatbot Interface (CLI)
def chatbot():
    print("Welcome to the Student Query Chatbot! (Type 'exit' to quit)")
    while True:
        query = input("Enter your query: ")
        if query.lower() == 'exit':
            break
        
        query_embedding = get_query_embedding(query)
        best_idx = find_best_match(query_embedding)
        best_text = student_texts[best_idx]
        response = extract_answer(query, best_text)
        print(f"Answer: {response}")

# Run the chatbot
if __name__ == "__main__":
    chatbot()



import torch
from transformers import BertTokenizer, BertForQuestionAnswering
import pandas as pd

# Sample DataFrame (replace this with your actual DataFrame)
data = {
    'Name': ['Alice', 'Bob', 'Charlie'],
    'Age': [20, 22, 19],
    'Course': ['Math', 'Physics', 'Chemistry'],
    'Marks': [85, 90, 78],
    'Grade': ['A', 'A+', 'B']
}
df = pd.DataFrame(data)

# Convert table to searchable text format (context for BERT)
student_texts = [
    f"{row['Name']} is {row['Age']} years old, enrolled in {row['Course']}, scored {row['Marks']} marks, and has grade {row['Grade']}."
    for _, row in df.iterrows()
]

# Combine all student texts into a single context
context = " ".join(student_texts)

# Load pre-trained BERT model and tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')
model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')

# Function to answer questions
def answer_question(question, context):
    # Tokenize the question and context
    inputs = tokenizer.encode_plus(question, context, add_special_tokens=True, return_tensors="pt", truncation=True, max_length=512)
    
    # Get input IDs, token type IDs, and attention mask
    input_ids = inputs["input_ids"]
    attention_mask = inputs["attention_mask"]
    
    # Run the model
    with torch.no_grad():
        outputs = model(input_ids, attention_mask=attention_mask)
        answer_start_scores = outputs.start_logits
        answer_end_scores = outputs.end_logits
    
    # Find the start and end of the answer
    answer_start = torch.argmax(answer_start_scores)
    answer_end = torch.argmax(answer_end_scores) + 1
    
    # Convert tokens to string
    answer_tokens = input_ids[0, answer_start:answer_end]
    answer = tokenizer.decode(answer_tokens, skip_special_tokens=True)
    
    return answer

# Example usage
questions = [
    "How old is Alice?",
    "What course is Bob enrolled in?",
    "What grade did Charlie get?"
]

for question in questions:
    answer = answer_question(question, context)
    print(f"Question: {question}")
    print(f"Answer: {answer}\n")