{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f6536ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.1272\n",
      "Epoch 2, Loss: 0.0906\n",
      "Epoch 3, Loss: 0.0810\n",
      "Epoch 4, Loss: 0.0753\n",
      "Epoch 5, Loss: 0.0715\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# RBM model definition\n",
    "class RBM(nn.Module):\n",
    "    def __init__(self, n_visible=784, n_hidden=500):\n",
    "        super(RBM, self).__init__()\n",
    "        self.W = nn.Parameter(torch.randn(n_hidden, n_visible) * 0.1)\n",
    "        self.h = nn.Parameter(torch.zeros(n_hidden))\n",
    "        self.v = nn.Parameter(torch.zeros(n_visible))\n",
    "    \n",
    "    def v_to_h(self, v):\n",
    "        h_probs = torch.sigmoid((v @ self.W.T) + self.h)\n",
    "        h_value = torch.bernoulli(h_probs)\n",
    "        return h_value, h_probs\n",
    "\n",
    "    def h_to_v(self, h):\n",
    "        v_probs = torch.sigmoid((h @ self.W) + self.v)\n",
    "        v_value = torch.bernoulli(v_probs)\n",
    "        return v_value, v_probs\n",
    "\n",
    "    def forward(self, v):\n",
    "        h_value, h_probs = self.v_to_h(v)\n",
    "        v_value, v_probs = self.h_to_v(h_value)\n",
    "        return v_value\n",
    "\n",
    "    def contrastive_divergence(self, v, lr=0.01):\n",
    "        h_value, h_probs = self.v_to_h(v)\n",
    "        v_value, v_probs = self.h_to_v(h_value)\n",
    "        h_value2, h_probs2 = self.v_to_h(v_probs)\n",
    "\n",
    "        # Update parameters\n",
    "        self.W.data += lr * ((h_probs.T @ v) - (h_probs2.T @ v_probs)) / v.size(0)\n",
    "        self.v.data += lr * (v - v_probs).mean(0)\n",
    "        self.h.data += lr * (h_probs - h_probs2).mean(0)\n",
    "\n",
    "# Load MNIST\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Lambda(lambda x: x.view(-1))])\n",
    "train_data = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "\n",
    "# Initialize model\n",
    "rbm = RBM(n_visible=784, n_hidden=256)\n",
    "\n",
    "# Training\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    for batch, _ in train_loader:\n",
    "        batch = batch.bernoulli()  # Binarize input\n",
    "        rbm.contrastive_divergence(batch)\n",
    "        loss = torch.mean((batch - rbm(batch)) ** 2)\n",
    "        epoch_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {epoch_loss / len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "beda5217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Starting pre-training...\n",
      "Pre-training RBM layer 1/2\n",
      "Epoch 1/10, Loss: 0.0213\n",
      "Epoch 2/10, Loss: 0.0133\n",
      "Epoch 3/10, Loss: 0.0116\n",
      "Epoch 4/10, Loss: 0.0108\n",
      "Epoch 5/10, Loss: 0.0103\n",
      "Epoch 6/10, Loss: 0.0100\n",
      "Epoch 7/10, Loss: 0.0097\n",
      "Epoch 8/10, Loss: 0.0096\n",
      "Epoch 9/10, Loss: 0.0094\n",
      "Epoch 10/10, Loss: 0.0093\n",
      "Pre-training RBM layer 2/2\n",
      "Epoch 1/10, Loss: 0.0453\n",
      "Epoch 2/10, Loss: 0.0296\n",
      "Epoch 3/10, Loss: 0.0267\n",
      "Epoch 4/10, Loss: 0.0253\n",
      "Epoch 5/10, Loss: 0.0242\n",
      "Epoch 6/10, Loss: 0.0233\n",
      "Epoch 7/10, Loss: 0.0227\n",
      "Epoch 8/10, Loss: 0.0220\n",
      "Epoch 9/10, Loss: 0.0214\n",
      "Epoch 10/10, Loss: 0.0209\n",
      "Starting fine-tuning...\n",
      "Epoch 1/10, Loss: 1.8260, Test Accuracy: 89.52%\n",
      "Epoch 2/10, Loss: 1.6188, Test Accuracy: 91.29%\n",
      "Epoch 3/10, Loss: 1.5880, Test Accuracy: 92.13%\n",
      "Epoch 4/10, Loss: 1.5736, Test Accuracy: 92.73%\n",
      "Epoch 5/10, Loss: 1.5644, Test Accuracy: 93.09%\n",
      "Epoch 6/10, Loss: 1.5578, Test Accuracy: 93.39%\n",
      "Epoch 7/10, Loss: 1.5527, Test Accuracy: 93.63%\n",
      "Epoch 8/10, Loss: 1.5485, Test Accuracy: 93.68%\n",
      "Epoch 9/10, Loss: 1.5449, Test Accuracy: 93.93%\n",
      "Epoch 10/10, Loss: 1.5418, Test Accuracy: 94.01%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Define device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define RBM class\n",
    "class RBM(nn.Module):\n",
    "    def __init__(self, visible, hidden):\n",
    "        super(RBM, self).__init__()\n",
    "        self.W = nn.Parameter(torch.randn(hidden, visible) * 0.1)\n",
    "        self.h_bias = nn.Parameter(torch.zeros(hidden))\n",
    "        self.v_bias = nn.Parameter(torch.zeros(visible))\n",
    "        \n",
    "    def sample_h(self, v):\n",
    "        h_prob = torch.sigmoid(F.linear(v, self.W, self.h_bias))\n",
    "        return h_prob, torch.bernoulli(h_prob)\n",
    "        \n",
    "    def sample_v(self, h):\n",
    "        v_prob = torch.sigmoid(F.linear(h, self.W.t(), self.v_bias))\n",
    "        return v_prob, torch.bernoulli(v_prob)\n",
    "   \n",
    "    def contrastive_derivative(self, v, lr=0.1):\n",
    "        v = v.to(device)\n",
    "        h_prob, h_sample = self.sample_h(v)\n",
    "        v_prob, _ = self.sample_v(h_sample)\n",
    "        h_prob_neg, _ = self.sample_h(v_prob)\n",
    " \n",
    "        self.W.data += lr * (torch.matmul(h_prob.t(), v) - torch.matmul(h_prob_neg.t(), v_prob)) / v.size(0)\n",
    "        self.v_bias.data += lr * torch.sum(v - v_prob, dim=0) / v.size(0)\n",
    "        self.h_bias.data += lr * torch.sum(h_prob - h_prob_neg, dim=0) / v.size(0)\n",
    "        return torch.mean((v - v_prob) ** 2)\n",
    "\n",
    "# Define DBN class\n",
    "class DBN(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super(DBN, self).__init__()\n",
    "        self.rbm = nn.ModuleList([RBM(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(layers[-1], 10),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        self.to(device)\n",
    "        \n",
    "    def pre_train(self, data, epochs=10, batch_size=100):\n",
    "        images = data.data.float() / 255.0  # Normalize to [0, 1]\n",
    "        images = images.view(-1, images.size(1) * images.size(2)).to(device)\n",
    "        for i, rbm in enumerate(self.rbm):\n",
    "            rbm.to(device)\n",
    "            print(f\"Pre-training RBM layer {i+1}/{len(self.rbm)}\")\n",
    "            for epoch in range(epochs):\n",
    "                epoch_loss = 0\n",
    "                for batch in DataLoader(images, batch_size=batch_size, shuffle=True):\n",
    "                    batch = batch.view(batch.size(0), -1).to(device)\n",
    "                    loss = rbm.contrastive_derivative(batch)\n",
    "                    epoch_loss += loss.item()\n",
    "                print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss/len(DataLoader(images, batch_size=batch_size)):.4f}\")\n",
    "            h_prob, _ = rbm.sample_h(images)\n",
    "            images = h_prob  # Pass hidden activations to next RBM\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1).to(device)\n",
    "        for rbm in self.rbm:\n",
    "            h_prob, _ = rbm.sample_h(x)\n",
    "            x = h_prob\n",
    "        return self.classifier(x)\n",
    "   \n",
    "    def finetune(self, train_data, test_data, epochs=10, batch_size=64):\n",
    "        optimizer = optim.SGD(self.parameters(), lr=0.1)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "        test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            self.train()\n",
    "            epoch_loss = 0\n",
    "            for images, targets in train_loader:\n",
    "                images, targets = images.to(device), targets.to(device)\n",
    "                images = images.view(images.size(0), -1)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.forward(images)\n",
    "                loss = criterion(outputs, targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                epoch_loss += loss.item()\n",
    "            \n",
    "            # Evaluate on test set\n",
    "            self.eval()\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            with torch.no_grad():\n",
    "                for images, targets in test_loader:\n",
    "                    images, targets = images.to(device), targets.to(device)\n",
    "                    images = images.view(images.size(0), -1)\n",
    "                    outputs = self.forward(images)\n",
    "                    _, predicted = torch.max(outputs, 1)\n",
    "                    total += targets.size(0)\n",
    "                    correct += (predicted == targets).sum().item()\n",
    "            \n",
    "            accuracy = 100 * correct / total\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss/len(train_loader):.4f}, Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# Load MNIST dataset\n",
    "transform = transforms.ToTensor()\n",
    "train_dataset = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
    "\n",
    "# Initialize DBN\n",
    "layers = [784, 500, 200]  # Visible: 784 (28x28), Hidden: 500, 200\n",
    "dbn = DBN(layers)\n",
    "\n",
    "# Pre-train DBN\n",
    "print(\"Starting pre-training...\")\n",
    "dbn.pre_train(train_dataset)\n",
    "\n",
    "# Fine-tune DBN\n",
    "print(\"Starting fine-tuning...\")\n",
    "dbn.finetune(train_dataset, test_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
